{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hate_speach_pre-processing_training_data_development_modeling.ipynb",
      "private_outputs": true,
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/natakatch/Springboard/blob/main/hate_speach_pre_processing_training_data_development_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGImbN-LtxNK"
      },
      "source": [
        "CAPSTONE Three."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV-1CZU1txNO"
      },
      "source": [
        "# Hate Speech and Offensive Language detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxZHCiihtxNO"
      },
      "source": [
        "## Pre-processing and Training Data Development, Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxoNjKj7txNP"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jo6WEz59txNP"
      },
      "source": [
        "Text classification is one of the major problems people are now days looking to solve. We will use supervised machine learning algoritms and NLP modeling for our hate-speech detection research; we will be using Twitter data. \n",
        "The text is already classified as: \n",
        " * hate-speech, \n",
        " * offensive language, \n",
        " * and neither. \n",
        "\n",
        "Word embeddings (algorithm for obtaining vector representations for words)\n",
        "  - Supervised with NLP - bag of words of models od tf-ids features\n",
        "  - Unsupervised learning GloVe.\n",
        "\n",
        "- Use Multinomial Naive Bayes algorithm is a probabilistic learning method \n",
        "- Random Forest\n",
        "- \n",
        "- Use Bert in multi label Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kx6QoQotxNT"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQYRSaqHtxNT"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "from scipy import stats\n",
        "# random enables us to generate random numbers\n",
        "import random\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq9U6eOhtxNT"
      },
      "source": [
        "from sklearn import tree, metrics\n",
        "from io import StringIO\n",
        "from IPython.display import Image  \n",
        "import pydotplus\n",
        "\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.metrics import roc_curve,roc_auc_score\n",
        "from sklearn.metrics import accuracy_score,log_loss\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix, classification_report\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7PkNcxztxNU"
      },
      "source": [
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynyg0Wm2txNV"
      },
      "source": [
        "import sys\n",
        "import torch\n",
        "\n",
        "from torch import nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MUlYSnztxNV"
      },
      "source": [
        "##  Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuH0Ldvbu8jC"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ufiCgM5txNV"
      },
      "source": [
        "Load the Hate Speach and Offensive language dataset (labeled_data.csv)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VwQ2uzTtxNV"
      },
      "source": [
        "hate_speach = pd.read_csv('labeled_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuYN32oytxNV"
      },
      "source": [
        "hate_speach"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4qZ2Tm-txNW"
      },
      "source": [
        "hate_speach.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhedFHontxNW"
      },
      "source": [
        "hate_speach.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InBmuJfJtxNW"
      },
      "source": [
        " # 4. Pre-processing and Training Data Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pat9MtfftxNW"
      },
      "source": [
        "The text is classified as: hate-speech, offensive language, and neither.\n",
        "* hate speech it is labelled as 0\n",
        "* offensive it is labelled as 1\n",
        "* neither then it is class 2\n",
        "\n",
        "The dataset has 7 columns. We will drop all the columns that contailn numbers of CF users who judged the tweet and keep only 2 columns: the tweet and class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8wi3i7dtxNW"
      },
      "source": [
        "hs = hate_speach[['class', 'tweet']]\n",
        "hs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKLnDNcHtxNW"
      },
      "source": [
        "# Grouping data by label\n",
        "hs.groupby('class').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRSS1WiW2hNq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsjAcwAftxNW"
      },
      "source": [
        " def percentage(part, whole):\n",
        "  Percentage = round(100 * float(part)/float(whole),2)\n",
        "  return str(Percentage) + \"%\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEAi_X6utxNW"
      },
      "source": [
        "hate, offensive, neither = np.bincount(hs['class'])\n",
        "whole = hate + offensive + neither\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLGCKUhdtxNX"
      },
      "source": [
        "hate_pr = percentage(hate,whole)\n",
        "offensive_pr = percentage(offensive,whole)\n",
        "neither_pr = percentage(neither,whole)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln0BKedxtxNX"
      },
      "source": [
        "print(\"Total : \" + str(whole))\n",
        "print()\n",
        "print(\"Hate : \" + hate_pr + \" of total\")\n",
        "print(\"Offensive : \" + offensive_pr + \" of total\")\n",
        "print(\"Neither : \" + neither_pr + \" of total\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPiKzrlEtxNX"
      },
      "source": [
        "This is an unbalanced dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quocOxDstxNZ"
      },
      "source": [
        "import seaborn as sn\n",
        "sn.distplot(hs['class'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-vRG5K42WYI"
      },
      "source": [
        "hs['class'].hist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rhiWoIKtxNZ"
      },
      "source": [
        "### 4.1 Create dummy or indicator features for categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph8WUvt6txNZ"
      },
      "source": [
        "tweet = hs.drop('class', axis=1)\n",
        "categories = hs['class']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej5u20vdtxNZ"
      },
      "source": [
        "Although some machine learning algorithms can interpret multi-level categorical variables, many machine learning models cannot handle categorical variables unless they are converted to dummy variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-qqsdgMtxNZ"
      },
      "source": [
        "cat = pd.get_dummies(categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhFkcyCVtxNa"
      },
      "source": [
        "where hate speech it is labelled as 0, offensive it is labelled as 1 \n",
        "neither then it is class 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMhZz5HetxNa"
      },
      "source": [
        "cat.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IxB7p7xtxNa"
      },
      "source": [
        "df = pd.concat([hs.drop(\"class\", axis=1), pd.get_dummies(categories)], axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk9LwiJZWq9"
      },
      "source": [
        "## Preprocessing of the tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpP7GxkcZgG8"
      },
      "source": [
        "* Removal of punctuation and capitlization\n",
        "* Tokenizing\n",
        "* Removal of stopwords\n",
        "* Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y9Q4T9mtxNa"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer() \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idi7QPJ7txNa"
      },
      "source": [
        "def preprocess(sentence):\n",
        "    sentence=str(sentence)\n",
        "    sentence = sentence.lower()\n",
        "    sentence=sentence.replace('{html}',\"\") \n",
        "    cleanr = re.compile('<.*?>@')\n",
        "    cleantext = re.sub(cleanr, '', sentence)\n",
        "    rem_url=re.sub(r'http\\S+', '',cleantext)\n",
        "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens = tokenizer.tokenize(rem_num)  \n",
        "    filtered_words = [w for w in tokens if len(w) > 2 if not w in stopwords.words('english')]\n",
        "    stem_words=[stemmer.stem(w) for w in filtered_words]\n",
        "    lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
        "    return \" \".join(filtered_words)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEUqeiUQtxNa"
      },
      "source": [
        "df['cleanTweet']=df['tweet'].map(lambda s:preprocess(s)) \n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "df['tokenized_tweets'] = df['cleanTweet'].apply(tweet_tokenizer.tokenize)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrxmtTwatxNa"
      },
      "source": [
        "df = df.drop(\"tweet\", axis=1)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sIWm4ORZuDe"
      },
      "source": [
        "### Save clean tweens in a new file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwHyuQCStxNa"
      },
      "source": [
        "#df.to_csv('../data/clean_tweets.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4eZiLrKtxNa"
      },
      "source": [
        "hs = hs.assign(cleanTweet=hs['tweet'].map(lambda s:preprocess(s))) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oewpkh-gwncQ"
      },
      "source": [
        "hs = hs.drop(\"tweet\", axis=1)\n",
        "hs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pC3U62PtxNa"
      },
      "source": [
        "hs.rename(columns={'class' : 'label'}, inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiQC0jlCtxNa"
      },
      "source": [
        "### 4.2. Standardize the magnitude of numeric features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkyx2fz1txNb"
      },
      "source": [
        "This is not applied in our situation. We don't have numeric features with different magnitude. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SGvULBLtxNb"
      },
      "source": [
        "## Visualization . Word Clouds "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4eNb42btxNb"
      },
      "source": [
        "Word Clouds are a popular way of displaying how important words are in a collection of texts. \n",
        "The more frequent the word is, the greater space it occupies in the image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3DrO9OltxNb"
      },
      "source": [
        "### Offensive language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKxQMi87txNb"
      },
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "stopwords = set(STOPWORDS)\n",
        "stopwords.add(\"RT\")\n",
        "\n",
        "print(type(STOPWORDS))\n",
        "\n",
        "import random\n",
        "\n",
        "def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n",
        "    h = 250\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stopwords,\n",
        "                          max_words=200,\n",
        "                          max_font_size=60, \n",
        "                          random_state=42\n",
        "                         ).generate(str(hs.loc[hs[\"label\"]==1].cleanTweet))\n",
        "print(wordcloud)\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n",
        "           interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PhqcFdvtxNb"
      },
      "source": [
        "### Hate language"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7EUQrmItxNb"
      },
      "source": [
        "stopwords.add(\"Name\")\n",
        "\n",
        "def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n",
        "    h = 344\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stopwords,\n",
        "                          max_words=200,\n",
        "                          max_font_size=60, \n",
        "                          random_state=42\n",
        "                         ).generate(str((hs.loc[hs[\"label\"]==0].cleanTweet)))\n",
        "print(wordcloud)\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n",
        "           interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqKSLzzfaAUQ"
      },
      "source": [
        "### Neither\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UH-sSRNtaOAW"
      },
      "source": [
        "stopwords.add(\"Name\")\n",
        "\n",
        "def random_color_func(word=None, font_size=None, position=None,  orientation=None, font_path=None, random_state=None):\n",
        "    h = 120\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(60, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "                          background_color='white',\n",
        "                          stopwords=stopwords,\n",
        "                          max_words=200,\n",
        "                          max_font_size=60, \n",
        "                          random_state=42\n",
        "                         ).generate(str((hs.loc[hs[\"label\"]==2].cleanTweet)))\n",
        "print(wordcloud)\n",
        "fig = plt.figure(1)\n",
        "plt.imshow(wordcloud.recolor(color_func= random_color_func, random_state=3),\n",
        "           interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hMkDK8btxNb"
      },
      "source": [
        "# 5 Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Vjp8YLtxNb"
      },
      "source": [
        "### Word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PScGIUd0txNb"
      },
      "source": [
        "#### Bag of words and Tf-idf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJDyQIDDtxNb"
      },
      "source": [
        "Use Bag-of-words vectors (build in scikit-learn) to predict the text label(category) based on the tweets wording.\n",
        "\n",
        "The data set we extracted has categorical features we generated using preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AG4mPzAktxNb"
      },
      "source": [
        "The BoW method is simple and works well, but it treats all words equally and cannot distinguish very common words or rare words. Tf-idf solves this problem of BoW Vectorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f51Qe6NutxNb"
      },
      "source": [
        "Both Vectorization techniques, BoW and tf-idf work well but it fails to suggest a relation between two words. Vectorization using word embedding solves this problem. We will discuss word embedding "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn3o1T9ftxNc"
      },
      "source": [
        "hs\n",
        "#hs.groupby('label').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfYsaOdUtxNc"
      },
      "source": [
        "#target\n",
        "y = hs.label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RR5IzTortxNc"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(hs[\"cleanTweet\"], y,test_size=0.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXWOl72dtxNc"
      },
      "source": [
        "#### CountVectorizer for text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUtOscEutxNc"
      },
      "source": [
        "count_vectorizer = CountVectorizer(stop_words=\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEqZ8ehmtxNc"
      },
      "source": [
        "# Transform the training data using only the 'tweet' column values: count_train \n",
        "count_train = count_vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using only the 'text' column values: count_test \n",
        "count_test = count_vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ-ux1-CtxNc"
      },
      "source": [
        "#### TfidfVectorizer for text classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbUXA-m2txNc"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\", max_df=0.7)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNoyXrGHtxNc"
      },
      "source": [
        "### Inspecting the vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4mR93CPtxNc"
      },
      "source": [
        "# tfidf_vectorizer the first 10 features\n",
        "tfidf_vectorizer.get_feature_names()[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvuM9_uVtxNc"
      },
      "source": [
        "# The first 5 vectors of the tfidf training data\n",
        "tfidf_train.A[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiJ9RoyYtxNc"
      },
      "source": [
        "#### Create the CountVectorizer and TfidfVectorizer dataframes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9hy6_mFtxNc"
      },
      "source": [
        "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
        "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names() )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey5dH2RBtxNc"
      },
      "source": [
        "#### CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4aWubJEtxNc"
      },
      "source": [
        "count_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMQUyUJntxNc"
      },
      "source": [
        "#### TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fqYrfPdtxNc"
      },
      "source": [
        "tfidf_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecx128i1txNd"
      },
      "source": [
        "### Calculate the difference in columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yq0KGdPtxNd"
      },
      "source": [
        "difference = set(count_df.columns) - set(tfidf_df.columns)\n",
        "difference"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWj2f4oPtxNd"
      },
      "source": [
        "### Check whether the DataFrames are equal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSlU4quctxNd"
      },
      "source": [
        "count_df.equals(tfidf_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFeXmBhHtxNd"
      },
      "source": [
        "hs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUDSYGtXtxNd"
      },
      "source": [
        "## Naive Bayes model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOPgBVIutxNd"
      },
      "source": [
        "Commonly used when training a new supervised model using text vector data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kPDu6_DtxNd"
      },
      "source": [
        "#### Train and test a Naive Bayes model using the CountVectorizer data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8b5rNFLtxNd"
      },
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "\n",
        "# Fit the classifier to the training data\n",
        "nb_classifier.fit(count_train, y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(count_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l7aC8N5txNd"
      },
      "source": [
        "#### Calculate the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHhyPlNAtxNd"
      },
      "source": [
        "score = metrics.accuracy_score(y_test, pred)\n",
        "score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nrahLqltxNd"
      },
      "source": [
        "#### Calculate the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWKkvqjrtxNd"
      },
      "source": [
        "cm = metrics.confusion_matrix(y_test,pred, labels=[0, 2])\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSpAdCt5txNd"
      },
      "source": [
        "plt.figure(figsize = (7,5))\n",
        "\n",
        "sns.heatmap(cm, annot=True, cmap ='Greens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtY3_rHLtxNd"
      },
      "source": [
        "#### Train and test a Naive Bayes model using the TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20870lpmtxNd"
      },
      "source": [
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(tfidf_train,y_train)\n",
        "\n",
        "# Create the predicted tags: pred\n",
        "pred = nb_classifier.predict(tfidf_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_P77MImFtxNd"
      },
      "source": [
        "# Calculate the accuracy score\n",
        "accuracy = metrics.accuracy_score(y_test, pred)\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pC9fLB2txNe"
      },
      "source": [
        "# Calculate the confusion matrix: cm\n",
        "cm = metrics.confusion_matrix(y_test, pred, labels=[0, 2])\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYssFXLptxNe"
      },
      "source": [
        "plt.figure(figsize = (7,5))\n",
        "sns.heatmap(cm, annot=True, cmap ='Greens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gx4py69txNe"
      },
      "source": [
        "## Classification with Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CgwTeD1txNe"
      },
      "source": [
        "The random forest algorithm works by completing the following steps:\n",
        "\n",
        "    * select random samples from the dataset provided.\n",
        "    * create a decision tree for each sample selected. Then it will get a prediction result from each decision tree created.\n",
        "    * voting will then be performed for every predicted result. For a classification problem, it will use mode, and for a regression problem, it will use mean.\n",
        "    * select the most voted prediction result as the final prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzEIzRmKtxNe"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyUXNF90txNe"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(hs[\"cleanTweet\"], y,test_size=0.25, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT93vx9-txNe"
      },
      "source": [
        "classifier_rf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
        "classifier_rf.fit(count_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i22HYbTBtxNe"
      },
      "source": [
        "y_pred_rf = classifier_rf.predict(count_test)\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Le4SK3wwtxNe"
      },
      "source": [
        "plt.figure(figsize = (7,5))\n",
        "sns.heatmap(cm, annot=True, cmap ='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrxLeD-htxNe"
      },
      "source": [
        "accuracy = metrics.accuracy_score(y_test, y_pred_rf)\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E7EDrEPtxNe"
      },
      "source": [
        "# Deep neural network for hate speech detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1edN-uMLtxNe"
      },
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHjwufjhtxNe"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEBP86-ctxNe"
      },
      "source": [
        "Tweets cannot be directly interpreted we need to transform them in a sequence of meaningful integer number in order to be analyzed by NN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sQ4_9nTtxNe"
      },
      "source": [
        "## GloVe Word Embeddings with Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kva-a6TDyglg"
      },
      "source": [
        "from scipy import spatial\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DzdFCUPy7uQ"
      },
      "source": [
        "Loading Glove Pre-trained Word Embedding Model from Text File\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw7Y1fKozajy"
      },
      "source": [
        "GloVe embeddings were trained on a dataset of one billion words with a vocabulary of 400 thousand words. There are few different embedding vector sizes such as 50, 100, 200 and 300 dimensions. We use 50 dimentions vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCirHFcs5eSf"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebSSRUYktxNf"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FC9T5zjtxNf"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df.cleanTweet)\n",
        "print('Found %d unique words.' % len(tokenizer.word_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evDUAqjzygpw"
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df.cleanTweet)\n",
        "X[:5] # first 5 elements of the X "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkVweY64ygtB"
      },
      "source": [
        "y = df[2] # column 2 - no hate or offensive language (0s - hate or offensive language)\n",
        "y.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bjc3b91k2_Hx"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "print(X[0])\n",
        "print(X[1])\n",
        "print()\n",
        "print(\"Max lenght of X\")\n",
        "max_length = max([len(s) for s in X]) #maximum length of all sequences. If not provided, sequences will be padded to the length of the longest individual sequence\n",
        "print(max_length)\n",
        "print()\n",
        "padded_seq = pad_sequences(X,maxlen=max_length,padding='post')\n",
        "print(padded_seq[0])\n",
        "print(padded_seq[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoYbZtpcH0tO"
      },
      "source": [
        "Preparing Training and Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms41KKPx5MpN"
      },
      "source": [
        "padded_seq,y = np.array(padded_seq),np.array(y)\n",
        "\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(padded_seq,y,test_size=0.25,stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8zzZ25ksbng"
      },
      "source": [
        "print (X_train.shape)\n",
        "print(y_train.shape)\n",
        "print (X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knv28gsz0Vft"
      },
      "source": [
        "MAX_NUM_WORDS = 100000\n",
        "EMBEDDING_DIM = 50\n",
        "MAX_SEQUENCE_LENGTH = 200\n",
        "VALIDATION_SPLIT = 0.2\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "len(word_index)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y--cGj4KuFJ"
      },
      "source": [
        "test_data = pad_sequences(X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "test_data.shape "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QySpEMB5M1K"
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('glove.6B.50d.txt',mode='rt',encoding='utf-8')\n",
        "import numpy as np\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    words = values[0]\n",
        "    coefs = np.asarray(values[1:],dtype='float32')\n",
        "    embeddings_index[words] = coefs\n",
        "f.close()\n",
        "print('Loaded word vectors',len(embeddings_index)) #Loaded word vectors 400000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR-PvMlN5M5k"
      },
      "source": [
        "\n",
        "\n",
        "# prepare embedding matrix - rows are the words from word_index, columns are the embeddings of that word from glove.\n",
        "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    if i > MAX_NUM_WORDS:\n",
        "        continue\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUx4L1KHU1XN"
      },
      "source": [
        "embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vxgbKMSm2VW"
      },
      "source": [
        "embedding_matrix.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cYGXbaIVjlg"
      },
      "source": [
        "## Simple NN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWW2ez_KVSPy"
      },
      "source": [
        "After the  pre-processing, the final dataset is ready to be analyzed. Due to the low dimensionality of the dataset, a simple NN model, with just an LSTM layer with 10 hidden units, will suffice the task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVOPfY3-Z9fm"
      },
      "source": [
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "# specify the maximum input length to the Embedding layer.\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32') \n",
        "\n",
        "# load these pre-trained word embeddings into an Embedding layer\n",
        "# note that we set trainable = False so as to keep the embeddings fixed\n",
        "embedding_layer = Embedding(num_words,\n",
        "                            EMBEDDING_DIM,\n",
        "                            embeddings_initializer=Constant(embedding_matrix),\n",
        "                            input_length=MAX_SEQUENCE_LENGTH,\n",
        "                            trainable=False)\n",
        "print(\"Preparing of embedding matrix is done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bv7VUcUVBUg"
      },
      "source": [
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding, LSTM\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "979VokWA5M8e"
      },
      "source": [
        "rnnmodel = Sequential()\n",
        "#rnnmodel.add(Embedding(MAX_NUM_WORDS, 128))\n",
        "rnnmodel.add(embedding_layer)\n",
        "rnnmodel.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "rnnmodel.add(Dense(2, activation='sigmoid'))\n",
        "rnnmodel.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sglYJ0B74nnN"
      },
      "source": [
        "rnnmodel.fit(X_train, Y_train,\n",
        "          batch_size=32, \n",
        "          epochs=1,\n",
        "          validation_split=0.2)\n",
        "\n",
        "score, acc = rnnmodel.evaluate(X_test, Y_test,\n",
        "                            batch_size=32)\n",
        "print('Test accuracy with RNN:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWDAu2ootxNf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jUIY20JtxNf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}